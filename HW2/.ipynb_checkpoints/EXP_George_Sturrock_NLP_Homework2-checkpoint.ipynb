{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Scoring of Graded Readers\n",
    "## DS 7337 - Natural Language Processing - Homework 2\n",
    "### George C. Sturrock\n",
    "### January 26, 2019\n",
    "#### Homework 2 Objectives\n",
    "1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. \n",
    "2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts used in homework 1.\n",
    "\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux-4.15.0-1026-gcp-x86_64-with-debian-stretch-sid\n",
      "Python: 3.5.6 |Anaconda custom (64-bit)| (default, Aug 26 2018, 21:41:56) \n",
      "[GCC 7.3.0]\n",
      "Requests: 2.19.1\n",
      "NLTK Version: 3.3\n",
      "Numpy Version: 1.15.2\n",
      "Pandas Version: 0.23.4\n",
      "Seaborn Version: 0.9.0\n",
      "BeatifulSoup Version: 4.6.3\n"
     ]
    }
   ],
   "source": [
    "import platform; print(\"Platform:\", platform.platform())\n",
    "import os\n",
    "import sys; print(\"Python:\", sys.version)\n",
    "import requests; print(\"Requests:\", requests.__version__)\n",
    "\n",
    "import nltk; print(\"NLTK Version:\", nltk.__version__)\n",
    "from nltk.corpus import stopwords\n",
    "theStopWords = set(stopwords.words('english'))\n",
    "\n",
    "import numpy as np; print(\"Numpy Version:\", np.__version__)\n",
    "import pandas as pd; print(\"Pandas Version:\", pd.__version__)\n",
    "import matplotlib.pyplot as plt; #print(\"Matplotlib Pyplot Version:\", plt.__version__)\n",
    "import string; #print(\"String Version:\", string.__version__)\n",
    "\n",
    "import seaborn as sns; print(\"Seaborn Version:\", sns.__version__)\n",
    "sns.set(style='darkgrid')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bs4 import BeautifulSoup; print(\"BeatifulSoup Version: 4.6.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NLP Pipeline Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Do:\n",
    "1.  Remove licensing\n",
    "2.  find correct a hrefs and automatically build list of links and download books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pipeline(url):\n",
    "    #Download source web page, strip HTML if necessary, trim to desired content\n",
    "    html = urlopen(url[0]).read()\n",
    "    html = html.decode(\"utf-8\")\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text(strip=True).lower()\n",
    "    position, raw_text_without_license = remove_gutenberg_license(raw)\n",
    "    \n",
    "    #tokenize the text including the header information\n",
    "    tokens = nltk.work_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\" \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\\n<!--\\n\\nDON\\'T USE THIS PAGE FOR SCRAPING.\\n\\nSeriously. You\\'ll only get your IP blocked.\\n\\nRead http://www.gutenberg.org/feeds/ to learn how to download Project\\nGutenberg metadata much faster than by scraping.\\n\\n--><html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:dcterms=\"http://purl.org/dc/terms/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:ebook=\"http://www.gutenberg.org/ebooks/\" xmlns:marcrel=\"http://www.loc.gov/loc.terms/relators/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\" lang=\"en_US\" xml:lang=\"en_US\" version=\"XHTML+RDFa 1.0\">\\n\\n\\n\\n\\n<head xmlns:og=\"http://opengraphprotocol.org/schema/\" profile=\"http://a9.com/-/spec/opensearch/1.1/\">\\n<style type=\"text/css\">\\n.icon   { background: transparent url(/pics/sprite.png?1544106575) 0 0 no-repeat; }\\n</style>\\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/pg-desktop-one.css?1544106575\" />\\n<script type=\"text/javascript\">//<!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open one text\n",
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/ebooks/15659\" #The beacon second reader\n",
    "html = request.urlopen(url).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'raw6' is not defined\n",
      "['icon', 'background', 'transparent', 'url', 'picsspritepng', 'norepeat', 'CDATA', 'var', 'jsonsearch', 'ebookssuggest', 'var', 'mobileurl', 'var', 'canonicalurl', 'http', 'var', 'lang', 'enUS', 'var', 'fblang', 'enUS', 'FB', 'accepts', 'xxXX', 'var', 'msgloadmore', 'Load', 'More', 'var', 'pagemode', 'screen', 'var', 'dialogtitle', 'var', 'dialogmessage', 'The', 'Beacon', 'Second', 'Reader', 'James', 'H', 'Fassett', 'Free', 'Ebook', 'qrcode', 'background', 'transparent', 'url', 'norepeat', 'Project', 'Gutenberg', 'free', 'ebooks', 'The', 'Beacon', 'Second', 'Reader', 'James', 'H', 'Fassett', 'No', 'cover', 'available', 'Download', 'Bibrec', 'Bibliographic', 'Record', 'Author', 'Fassett', 'James', 'H', 'James', 'Hiram', 'Title', 'The', 'Beacon', 'Second', 'Reader', 'Language', 'English', 'LoC', 'Class', 'PE', 'Language', 'Literatures', 'English', 'Subject', 'Fairy', 'tales', 'Subject', 'Readers', 'Subject', 'Reading', 'Elementary', 'Category', 'Text', 'EBookNo', 'Release', 'Date', 'Apr', 'Copyright', 'Status', 'Public', 'domain', 'USA', 'Downloads', 'downloads', 'last', 'days', 'Price', 'Similar', 'Books', 'Readers', 'also', 'In', 'Children', 'Instructional', 'Books', 'Download', 'This', 'eBook', 'Format', 'Url', 'Size', 'Read', 'book', 'online', 'HTML', 'kB', 'EPUB', 'images', 'MB', 'EPUB', 'images', 'kB', 'Kindle', 'images', 'MB', 'Kindle', 'images', 'kB', 'Plain', 'Text', 'kB', 'More', 'http', 'http', 'Gutenberg', 'offers', 'free', 'ebooks', 'download', 'Project', 'Gutenberg', 'offers', 'free', 'ebooks', 'download', 'Search', 'Latest', 'Terms', 'Use', 'Donate', 'Mobile', 'Help', 'Enter', 'search', 'terms', 'separated', 'spaces', 'press', 'Enter', 'Avoid', 'punctuation', 'except', 'indicated', 'Suffixes', 'exact', 'match', 'Prefixes', 'author', 'title', 'subject', 'l', 'language', 'ebook', 'n', 'ebook', 'cat', 'category', 'Operators', 'Always', 'put', 'spaces', 'around', 'grouping', 'query', 'finds', 'shakespeare', 'hamlet', 'Hamlet', 'Shakespeare', 'qui', 'qui', 'Quixote', 'love', 'stories', 'love', 'stories', 'ashakespeare', 'Shakespeare', 'sshakespeare', 'Shakespeare', 'ebook', 'juvenile', 'lgerman', 'juvenile', 'lit', 'German', 'verne', 'lfr', 'lit', 'Verne', 'French', 'Italian', 'love', 'stories', 'austen', 'love', 'stories', 'Austen', 'jane', 'austen', 'cataudio', 'audio', 'books', 'Jane', 'Austen']\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "### Turn web page to text ###\n",
    "try:\n",
    "    del raw, raw2, raw3, raw4, raw5\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "#def toNLTK(textIn):\n",
    "#    raw = nltk.word_tokenize(textIn.read())\n",
    "#    punc = [r.translate(table) for r in raw]\n",
    "#    rnltk = nltk.Text(punc)\n",
    "#    return rnltk;\n",
    "\n",
    "#get text out of HTML\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "raw2 = nltk.word_tokenize(raw)\n",
    "raw3 = [r.translate(table) for r in raw2]\n",
    "raw4 = nltk.Text(raw3)\n",
    "raw5 = raw4.tokens\n",
    "\n",
    "text = []\n",
    "#nbrs = range(0, 999999, 1)\n",
    "#remove stopwords, blanks and other \n",
    "for r in raw5:\n",
    "    if r not in theStopWords:\n",
    "        if r not in '':\n",
    "            if r.isalpha():\n",
    "                text.append(r)\n",
    "            \n",
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
