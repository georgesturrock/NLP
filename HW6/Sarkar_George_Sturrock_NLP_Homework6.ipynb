{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Phrase Chunking of IMDB Movie Reviews\n",
    "### DS 7337 - Natural Laguage Processing - Homework 6\n",
    "#### George C. Sturrock\n",
    "##### March 16, 2019\n",
    "##### Homework 6 Objectives\n",
    "1.  Evaluate text similarity of Amazon book search results by doing the following:\n",
    "\n",
    "a.  Do a book search on Amazon. Manually copy the full book title (including subtitle) of each of the top 24 books listed in the first two pages of search results. \n",
    "\n",
    "b.  In Python, run one of the text-similarity measures covered in this course, e.g., cosine similarity. Compare each of the book titles, pairwise, to every other one. \n",
    "\n",
    "c.  Which two titles are the most similar to each other? Which are the most dissimilar? Where do they rank, among the first 24 results?\n",
    "\n",
    "\n",
    "2.  Now evaluate using a major search engine.\n",
    "\n",
    "a.  Enter one of the book titles from question 1a into Google, Bing, or Yahoo!. Copy the capsule of the first organic result and the 20th organic result. Take web results only (i.e., not video results), and skip sponsored results. \n",
    "\n",
    "b.  Run the same text similarity calculation that you used for question 1b on each of these capsules in comparison to the original query (book title). \n",
    "\n",
    "c.  Which one has the highest similarity measure? \n",
    "\n",
    "Submit all of your inputs and outputs and your code for this assignment, along with a brief written explanation of your findings. \n",
    "\n",
    "\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Windows-10-10.0.14393-SP0\n",
      "Python: 3.6.8 |Anaconda custom (64-bit)| (default, Feb 11 2019, 15:03:47) [MSC v.1915 64 bit (AMD64)]\n",
      "Requests: 2.21.0\n",
      "BeatifulSoup: 4.7.1\n",
      "json: 2.0.9\n",
      "Numpy Version: 1.15.4\n",
      "Pandas Version: 0.24.1\n",
      "NLTK Version: 3.4\n",
      "Spacy Version: 2.0.16\n",
      "en_core_web_md Version: 2.0.0\n",
      "Pattern Version: 3.6\n"
     ]
    }
   ],
   "source": [
    "import platform; print(\"Platform:\", platform.platform())\n",
    "import os\n",
    "import sys; print(\"Python:\", sys.version)\n",
    "import requests; print(\"Requests:\", requests.__version__)\n",
    "from urllib import request; (\"urllib:\", request.__version__)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from time import sleep\n",
    "\n",
    "#Web Scraping\n",
    "import bs4; print(\"BeatifulSoup:\", bs4.__version__)\n",
    "from bs4 import BeautifulSoup\n",
    "import json; print(\"json:\", json.__version__)\n",
    "\n",
    "#Python Basics\n",
    "import numpy as np; print(\"Numpy Version:\", np.__version__)\n",
    "import pandas as pd; print(\"Pandas Version:\", pd.__version__)\n",
    "import seaborn as sns;\n",
    "\n",
    "#NLP\n",
    "import nltk; print(\"NLTK Version:\", nltk.__version__)\n",
    "#nltk.download('conll2000')\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import spacy; print(\"Spacy Version:\", spacy.__version__)\n",
    "from spacy import displacy\n",
    "#nlp = spacy.load('en')\n",
    "#import en_core_web_sm; print(\"en_core_web_sm Version:\", en_core_web_sm.__version__)\n",
    "#nlp = en_core_web_sm.load()\n",
    "import en_core_web_md; print(\"en_core_web_md Version:\", en_core_web_md.__version__)\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "import pattern; print(\"Pattern Version:\", pattern.__version__)\n",
    "from pattern.en import tag\n",
    "from pattern.en import parsetree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1\n",
    "Despite Stephen Ambrose's propensity for plagiarism, \"Band of Brothers\" is one of my all-time favorite books.  An Amazon book search for this title returns related results for this title.  The results include several formats of the original book included paper back, Kindle, DVD, etc.  These redundant results were not included in the list below which includes the top twenty-five related results (not including the aforementioned duplicates).  Additionally, only book results were included in the list.  T-shirts appeared in the search results even though the Amazon book search was filtered for \"Books\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boblist = [\"Band of Brothers: E Company, 506th Regiment, 101st Airborne from Normandy to Hitler's Eagle's Nest\", \n",
    "          \"Beyond Band of Brothers: The War Memoirs of Major Dick Winters\", \n",
    "          \"Brothers in Battle, Best of Friends: Two WWII Paratroopers from the Original Band of Brothers Tell Their Story\",\n",
    "          \"Shifty's War: The Authorized Biography of Sergeant Darrell Powers, the Legendary Sharpshooter from the Band\", \n",
    "          \"Conversations with Major Dick Winters: Life Lessons from the Commander of the Band of Brothers\", \n",
    "          \"Easy Company Soldier: The Legendary Battles of a Sergeant from WW II's 'Band of Brothers'\",\n",
    "          \"How Easy Company Became a Band of Brothers\",\n",
    "          \"Biggest Brother: The Life Of Major Dick Winters, The Man Who Led The Band of Brothers\", \n",
    "          \"We Who Are Alive and Remain: Untold Stories from the Band of Brothers\", \n",
    "          \"The General's Driver: A Vietnam Soldier Remembers\", \n",
    "          \"Killing the SS: The Hunt for the Worst War Criminals in History (Bill O'Reilly's Killing Series)\", \n",
    "          \"The Allman Brothers Band Classic Memorabilia, 1969-76 (Music and the American South)\", \n",
    "          \"Building Your Band of Brothers\", \n",
    "          \"Killer Genius: The Bizarre Case of the Homicidal Scholar (Dead True Crime Book 5)\", \n",
    "          \"Murderer's Gulch: Carnage in the Catskills (Dead True Crime Book 4)\", \n",
    "          \"Band of Brothers: Battle Manual\", \n",
    "          \"Call of Duty: My Life Before, During and After the Band of Brothers\", \n",
    "          \"Band of Brothers: The Complete Campaigns\", \n",
    "          \"My Kid Brother's Band... a.k.a. The Beatles\", \n",
    "          \"One More Moment (Shaughnessy Brothers: Band on the Run Book 3)\", \n",
    "          \"One Way Out: The Inside History of the Allman Brothers Band\", \n",
    "          \"Tangled Up in You (The Shaughnessy Brothers Book 7)\", \n",
    "          \"The Allman Brothers Band: The Definitive Collection for Guitar, Vol. 3\", \n",
    "          \"The Allman Brothers Band - The Definitive Collection for Guitar - Volume 1 (Guitar Recorded Versions S)\", \n",
    "          \"Bastogne Band of Brothers Guide\", \n",
    "          \"Daisy Jones & The Six: A Novel\", \n",
    "          \"Band of Brothers: E Company 506th Regiment 101st Airborne, D-day & Citizen Soldiers Box Set\", \n",
    "          \"D DAY Through German Eyes - The Hidden Story of June 6th 1944\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarker p. 270\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type = 'frequency',\n",
    "                         ngram_range = (1, 1), min_df = 0.0, max_df = 1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary = True, min_df = min_df,\n",
    "                                     max_df = max_df, ngram_range = ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary = False, min_df = min_df,\n",
    "                                     max_df = max_df, ngram_range = ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df, \n",
    "                                     ngram_range = ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarker p. 287\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features, top_n = 3):\n",
    "\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    \n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    \n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-647192636b1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#html_parser = HTMLParser()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnorm_boblist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboblist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_boblist,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Sarkar p. 206\n",
    "from normalization import normalize_corpus\n",
    "\n",
    "\n",
    "# Sarker p. 286-87\n",
    "\n",
    "query_docs = boblist\n",
    "\n",
    "#html_parser = HTMLParser()\n",
    "\n",
    "norm_boblist = normalize_corpus(boblist, lemmatize = True)\n",
    "\n",
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_boblist,\n",
    "                                                        feature_type = 'tfidf',\n",
    "                                                        ngram_range = (1, 1), \n",
    "                                                        min_df = 0.0,\n",
    "                                                        max_df = 1.0)\n",
    "                                                        \n",
    "# normalize and extract features from the query boblist\n",
    "\n",
    "norm_query_docs =  normalize_boblist(query_docs, lemmatize = True)    \n",
    "  \n",
    "\n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\n",
    "\n",
    "# cosine similarity\n",
    "    \n",
    "print ('='*60)\n",
    "print ('Book Titles Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "print (\"\\n\")\n",
    "\n",
    "# return top 2 similar, then drop the 1st if same as primary document\n",
    "\n",
    "for index, doc in enumerate(query_docs):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    \n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf, tfidf_features, top_n = 27)\n",
    "    \n",
    "    if(top_similar_docs[0][0] == index) :\n",
    "        tsd = top_similar_docs[1]\n",
    "    else :\n",
    "        tsd = top_similar_docs[0]\n",
    "            \n",
    "    print ('-'*80)\n",
    "    buffer = \"Title %2d         : %s\" % (index, doc)\n",
    "    print(buffer)\n",
    "    print('Cosine similar   :', boblist[tsd[0]])\n",
    "    print('Similarity score :', tsd[1])\n",
    "    print ('-'*80) \n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
